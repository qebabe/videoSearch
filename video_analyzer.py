import cv2
import torch
import numpy as np
from pathlib import Path
from ultralytics import YOLO
import whisper
import openai
import json
import logging
from datetime import datetime
from tqdm import tqdm
import os
import sys
import subprocess
import hashlib
import dashscope
from http import HTTPStatus
from queue import Queue, Full
from threading import Thread
import easyocr
import traceback

class VideoAnalyzer:
    def __init__(self, use_gpu=True, openai_api_key=None, dashscope_api_key=None):
        """åˆå§‹åŒ–è§†é¢‘åˆ†æå™¨
        
        Args:
            use_gpu (bool): æ˜¯å¦ä½¿ç”¨GPUåŠ é€Ÿ
            openai_api_key (str): OpenAI APIå¯†é’¥
            dashscope_api_key (str): é€šä¹‰åƒé—® APIå¯†é’¥
        """
        self.logger = self._setup_logger()
        self.openai_api_key = openai_api_key
        self.dashscope_api_key = dashscope_api_key
        if openai_api_key:
            openai.api_key = openai_api_key
        if dashscope_api_key:
            dashscope.api_key = dashscope_api_key
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.temp_dir = Path("temp")
        self.temp_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        if use_gpu:
            os.environ['CUDA_VISIBLE_DEVICES'] = '0'
            
            # è¯Šæ–­ä¿¡æ¯
            self.logger.info("\nPyTorchç¯å¢ƒè¯Šæ–­:")
            self.logger.info(f"Pythonç‰ˆæœ¬: {sys.version}")
            self.logger.info(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
            
            # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
            if torch.cuda.is_available():
                self.device = torch.device("cuda:0")
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                self.logger.info(f"GPUå‹å·: {gpu_name}")
                self.logger.info(f"GPUæ€»å†…å­˜: {gpu_memory:.2f}GB")
            else:
                self.logger.warning("æœªæ£€æµ‹åˆ°å¯ç”¨çš„GPUï¼Œå°†ä½¿ç”¨CPUå¤„ç†")
                self.device = torch.device("cpu")
        else:
            self.device = torch.device("cpu")
            
        # åˆå§‹åŒ–æ¨¡å‹
        self._init_models()
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("VideoAnalyzer")
        logger.setLevel(logging.INFO)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger
        
    def _init_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰éœ€è¦çš„æ¨¡å‹"""
        self.logger.info("\nå¼€å§‹åŠ è½½æ¨¡å‹...")
        
        # åŠ è½½YOLOv8æ¨¡å‹ç”¨äºç‰©ä½“æ£€æµ‹
        try:
            self.object_detector = YOLO('yolov8n.pt')
            if isinstance(self.device, torch.device) and self.device.type == "cuda":
                self.object_detector.to(self.device)
            self.logger.info(f"YOLOv8æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {self.device}")
        except Exception as e:
            self.logger.error(f"YOLOv8æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.object_detector = None
            
        # åŠ è½½EasyOCRæ¨¡å‹ç”¨äºæ–‡å­—è¯†åˆ«
        try:
            use_gpu = isinstance(self.device, torch.device) and self.device.type == "cuda"
            self.ocr = easyocr.Reader(['ch_sim', 'en'], gpu=use_gpu)
            self.logger.info(f"EasyOCRæ¨¡å‹åŠ è½½æˆåŠŸï¼Œè®¾å¤‡: {'GPU' if use_gpu else 'CPU'}")
        except Exception as e:
            self.logger.error(f"EasyOCRæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.ocr = None
            
        # åŠ è½½Whisperæ¨¡å‹ç”¨äºéŸ³é¢‘è¯†åˆ«
        try:
            # è®¾ç½®æ¨¡å‹è·¯å¾„
            model_name = "medium"
            cache_dir = os.path.expanduser("~/.cache/whisper")
            if os.name == 'nt':  # Windowsç³»ç»Ÿ
                cache_dir = os.path.expanduser("~\\.cache\\whisper")
            
            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            os.makedirs(cache_dir, exist_ok=True)
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            model_path = os.path.join(cache_dir, f"{model_name}.pt")
            if not os.path.exists(model_path):
                self.logger.warning(f"Whisperæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                self.logger.info(f"è¯·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶å¹¶æ”¾ç½®åœ¨: {model_path}")
                self.logger.info("ä¸‹è½½é“¾æ¥:")
                self.logger.info("- tiny: https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt")
                self.logger.info("- base: https://openaipublic.azureedge.net/main/whisper/models/ed3a0b6b1c0edf879ad9b11b1af5a0e6ab5db9205f891f668f8b0e6c6326e34e/base.pt")
                self.logger.info("- small: https://openaipublic.azureedge.net/main/whisper/models/9ecf779972d90ba49c06d968637d720dd632c55bbf19d441fb42bf17a411e794/small.pt")
                self.logger.info("- medium: https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt")
                self.logger.info("- large: https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt")
            
            # åŠ è½½æ¨¡å‹
            self.audio_model = whisper.load_model(model_name)
            if isinstance(self.device, torch.device) and self.device.type == "cuda":
                self.audio_model.to(self.device)
            self.logger.info(f"Whisperæ¨¡å‹({model_name})åŠ è½½æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.audio_model = None
            
    def analyze_video(self, video_path):
        """åˆ†æè§†é¢‘å†…å®¹"""
        try:
            # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
            cap = cv2.VideoCapture(video_path)
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            
            # è®¡ç®—è§†é¢‘æ–‡ä»¶çš„MD5
            md5_hash = self._calculate_file_md5(video_path)
            
            # å‡†å¤‡ç»“æœæ•°æ®ç»“æ„
            results = {
                'metadata': {
                    'duration': duration,
                    'resolution': f"{width}x{height}",
                    'md5': md5_hash,
                    'file_path': str(video_path)
                },
                'results': []
            }
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç¼“å­˜çš„åˆ†æç»“æœ
            cache_dir = Path("cache")
            cache_dir.mkdir(exist_ok=True)
            
            visual_cache_file = cache_dir / f"{md5_hash}_visual_results.json"
            audio_cache_file = cache_dir / f"{md5_hash}_audio_results.json"
            
            # åˆ†æè§†è§‰å†…å®¹
            visual_results = []
            if visual_cache_file.exists():
                self.logger.info("å‘ç°è§†è§‰åˆ†æç¼“å­˜ï¼Œæ­£åœ¨åŠ è½½...")
                with open(visual_cache_file, 'r', encoding='utf-8') as f:
                    visual_results = json.load(f)
            else:
                visual_results = self._analyze_visual_content(video_path, frame_count)
                with open(visual_cache_file, 'w', encoding='utf-8') as f:
                    json.dump(visual_results, f, ensure_ascii=False, indent=2)
            
            # åˆ†æéŸ³é¢‘å†…å®¹
            audio_results = []
            if audio_cache_file.exists():
                self.logger.info("ä½¿ç”¨ç¼“å­˜çš„éŸ³é¢‘åˆ†æç»“æœ")
                with open(audio_cache_file, 'r', encoding='utf-8') as f:
                    audio_results = json.load(f)
            else:
                audio_results = self._analyze_audio_content(video_path)
                with open(audio_cache_file, 'w', encoding='utf-8') as f:
                    json.dump(audio_results, f, ensure_ascii=False, indent=2)
            
            # åˆå¹¶ç»“æœ
            results['results'] = self._merge_timeline_results(visual_results, audio_results)
            
            return results
            
        except Exception as e:
            self.logger.error(f"è§†é¢‘åˆ†æå¤±è´¥: {str(e)}")
            self.logger.error(traceback.format_exc())
            raise
        
    def _extract_metadata(self, video_path: str) -> dict:
        """æå–è§†é¢‘å…ƒæ•°æ®"""
        cap = cv2.VideoCapture(video_path)
        metadata = {
            "duration": cap.get(cv2.CAP_PROP_FRAME_COUNT) / cap.get(cv2.CAP_PROP_FPS),
            "resolution": f"{int(cap.get(3))}x{int(cap.get(4))}"
        }
        cap.release()
        return metadata
        
    def _get_cache_path(self, video_path: str, analysis_type: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„
        
        Args:
            video_path (str): è§†é¢‘æ–‡ä»¶è·¯å¾„
            analysis_type (str): åˆ†æç±»å‹ ('visual' æˆ– 'audio')
            
        Returns:
            Path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        video_md5 = self._calculate_file_md5(video_path)
        return self.cache_dir / f"{video_md5}_{analysis_type}_results.json"

    def _load_cache(self, cache_path: Path) -> dict:
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        try:
            if cache_path.exists():
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            self.logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {str(e)}")
        return None

    def _save_cache(self, cache_path: Path, data: dict):
        """ä¿å­˜ç¼“å­˜æ–‡ä»¶"""
        try:
            # è½¬æ¢æ•°æ®ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
            def convert_to_serializable(obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, (list, tuple)):
                    return [convert_to_serializable(item) for item in obj]
                elif isinstance(obj, dict):
                    return {key: convert_to_serializable(value) for key, value in obj.items()}
                return obj
            
            # è½¬æ¢æ•°æ®
            serializable_data = convert_to_serializable(data)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}")
            
    def _analyze_visual(self, video_path: str) -> dict:
        """åˆ†æè§†é¢‘çš„è§†è§‰å†…å®¹"""
        # åˆå§‹åŒ–ç©ºç»“æœ
        empty_results = {
            "objects": [],
            "texts": [],
            "metadata": {
                "total_frames": 0,
                "fps": 0,
                "width": 0,
                "height": 0,
                "sample_interval": 0
            }
        }
        
        # æ£€æŸ¥ç¼“å­˜
        cache_path = self._get_cache_path(video_path, "visual")
        if cache_path.exists():
            self.logger.info("å‘ç°è§†è§‰åˆ†æç¼“å­˜ï¼Œæ­£åœ¨åŠ è½½...")
            cached_results = self._load_cache(cache_path)
            if cached_results:
                return cached_results
            else:
                self.logger.warning("ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°åˆ†æ")
                
        self.logger.info("å¼€å§‹è§†è§‰å†…å®¹åˆ†æ...")
        
        # æ‰“å¼€è§†é¢‘æ–‡ä»¶
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            self.logger.error("æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            return empty_results
            
        # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # è®¾ç½®å¸§é‡‡æ ·é—´éš”ï¼ˆ1å¸§/ç§’ï¼‰
        sample_interval = max(1, int(fps))
        
        # åˆ›å»ºæ˜¾ç¤ºçª—å£
        cv2.namedWindow('Video Analysis', cv2.WINDOW_NORMAL)
        cv2.resizeWindow('Video Analysis', 1280, 720)
        
        # åˆå§‹åŒ–ç»“æœ
        results = {
            "objects": [],
            "texts": [],
            "metadata": {
                "total_frames": int(total_frames),
                "fps": float(fps),
                "width": int(frame_width),
                "height": int(frame_height),
                "sample_interval": int(sample_interval)
            }
        }
        
        try:
            frame_count = 0
            with tqdm(total=total_frames, desc="åˆ†æå¸§", position=1) as pbar:
                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break
                        
                    frame_count += 1
                    pbar.update(1)
                    
                    # æ¯ç§’é‡‡æ ·ä¸€å¸§è¿›è¡Œåˆ†æ
                    if frame_count % sample_interval != 0:
                        continue
                        
                    # ç‰©ä½“æ£€æµ‹
                    if self.object_detector:
                        try:
                            detections = self.object_detector(frame)[0]
                            for det in detections.boxes.data.tolist():
                                x1, y1, x2, y2, conf, cls = det
                                class_name = detections.names[int(cls)]
                                results["objects"].append({
                                    "frame": int(frame_count),
                                    "time": float(frame_count / fps),
                                    "class": str(class_name),
                                    "confidence": float(conf),
                                    "bbox": [float(x1), float(y1), float(x2), float(y2)]
                                })
                                
                                # åœ¨å¸§ä¸Šç»˜åˆ¶æ£€æµ‹æ¡†
                                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
                                cv2.putText(frame, f"{class_name}: {conf:.2f}", 
                                          (int(x1), int(y1) - 10), 
                                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                        except Exception as e:
                            self.logger.error(f"ç‰©ä½“æ£€æµ‹å¤±è´¥: {str(e)}")
                    
                    # æ–‡å­—è¯†åˆ«
                    if self.ocr:
                        try:
                            ocr_results = self.ocr.readtext(frame)
                            for detection in ocr_results:
                                bbox, text, conf = detection
                                if conf > 0.5:  # ä»…ä¿ç•™ç½®ä¿¡åº¦å¤§äº0.5çš„ç»“æœ
                                    # ç¡®ä¿è¾¹ç•Œæ¡†åæ ‡æ˜¯åŸç”ŸPythonç±»å‹
                                    bbox = [[float(x), float(y)] for x, y in bbox]
                                    results["texts"].append({
                                        "frame": int(frame_count),
                                        "time": float(frame_count / fps),
                                        "text": str(text),
                                        "confidence": float(conf),
                                        "bbox": bbox
                                    })
                                    
                                    # åœ¨å¸§ä¸Šç»˜åˆ¶æ–‡å­—æ¡†
                                    pts = np.array(bbox, np.int32)
                                    cv2.polylines(frame, [pts], True, (0, 0, 255), 2)
                                    cv2.putText(frame, text, 
                                              (int(bbox[0][0]), int(bbox[0][1]) - 10),
                                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                        except Exception as e:
                            self.logger.error(f"æ–‡å­—è¯†åˆ«å¤±è´¥: {str(e)}")
                    
                    # æ˜¾ç¤ºå¤„ç†åçš„å¸§
                    cv2.imshow('Video Analysis', frame)
                    
                    # æŒ‰'q'é”®é€€å‡º
                    if cv2.waitKey(1) & 0xFF == ord('q'):
                        break
                        
        except Exception as e:
            self.logger.error(f"è§†é¢‘åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            
        finally:
            cap.release()
            cv2.destroyAllWindows()
            
            # ä¿å­˜ç¼“å­˜ï¼ˆå³ä½¿å‘ç”Ÿé”™è¯¯ä¹Ÿä¿å­˜å·²åˆ†æçš„ç»“æœï¼‰
            if results["objects"] or results["texts"]:
                self._save_cache(cache_path, results)
            
            return results
            
    def _analyze_audio(self, video_path: str) -> dict:
        """ä½¿ç”¨Whisperåˆ†æè§†é¢‘çš„éŸ³é¢‘å†…å®¹"""
        # æ£€æŸ¥ç¼“å­˜
        cache_path = self._get_cache_path(video_path, 'audio')
        cached_results = self._load_cache(cache_path)
        if cached_results:
            self.logger.info("ä½¿ç”¨ç¼“å­˜çš„éŸ³é¢‘åˆ†æç»“æœ")
            return cached_results

        self.logger.info("\nå¼€å§‹éŸ³é¢‘åˆ†æ")
        
        try:
            # è®¡ç®—è§†é¢‘æ–‡ä»¶çš„MD5å€¼
            video_md5 = self._calculate_file_md5(video_path)
            audio_path = self.temp_dir / f"{video_md5}.wav"
            
            # æå–éŸ³é¢‘
            if not audio_path.exists():
                self.logger.info("æ­£åœ¨æå–éŸ³é¢‘...")
                cmd = [
                    'ffmpeg',
                    '-i', video_path,
                    '-vn',
                    '-acodec', 'pcm_s16le',
                    '-ac', '1',
                    '-ar', '16000',
                    '-y',
                    str(audio_path)
                ]
                
                try:
                    process = subprocess.Popen(
                        cmd,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        text=True,
                        encoding='utf-8',
                        errors='ignore'
                    )
                    _, stderr = process.communicate()
                    
                    if process.returncode != 0:
                        self.logger.error(f"FFmpegé”™è¯¯: {stderr}")
                        raise subprocess.CalledProcessError(process.returncode, cmd, stderr)
                        
                    self.logger.info(f"éŸ³é¢‘æå–æˆåŠŸ: {audio_path}")
                except subprocess.CalledProcessError as e:
                    self.logger.error(f"FFmpegé”™è¯¯: {e.stderr}")
                    raise
            else:
                self.logger.info(f"ä½¿ç”¨å·²å­˜åœ¨çš„éŸ³é¢‘æ–‡ä»¶: {audio_path}")
            
            # ä½¿ç”¨Whisperè¿›è¡ŒéŸ³é¢‘è¯†åˆ«
            if self.audio_model:
                result = self.audio_model.transcribe(
                    str(audio_path),
                    language='zh',  # ä½¿ç”¨'zh'æ¥æŒ‡å®šç®€ä½“ä¸­æ–‡
                    task='transcribe',
                    initial_prompt="è¿™æ˜¯ä¸€æ®µä¸­æ–‡å¹¿å‘Šå®£ä¼ ç‰‡çš„è§£è¯´è¯ã€‚è¯·å¿½ç•¥èƒŒæ™¯éŸ³ä¹ï¼Œåªè¾“å‡ºæ¸…æ™°çš„äººå£°å†…å®¹ã€‚",
                    temperature=0.0,  # å®Œå…¨æ¶ˆé™¤éšæœºæ€§
                    best_of=5,  # ç”Ÿæˆå¤šä¸ªå€™é€‰ç»“æœå¹¶é€‰æ‹©æœ€ä½³
                    beam_size=5,  # ä½¿ç”¨æ³¢æŸæœç´¢
                    condition_on_previous_text=True,  # è€ƒè™‘å‰æ–‡è¯­å¢ƒ
                    no_speech_threshold=0.6,  # æé«˜éè¯­éŸ³æ£€æµ‹é˜ˆå€¼
                    logprob_threshold=-1.0,  # æé«˜ç½®ä¿¡åº¦è¦æ±‚
                    compression_ratio_threshold=2.4,  # æ§åˆ¶è¾“å‡ºæ–‡æœ¬çš„å‹ç¼©æ¯”
                    word_timestamps=True  # å¯ç”¨è¯çº§æ—¶é—´æˆ³ä»¥æ›´å¥½åœ°è¿‡æ»¤å™ªéŸ³
                )
                
                # æ¸…ç†å’Œè¿‡æ»¤æ–‡æœ¬
                def clean_text(text):
                    # ç§»é™¤éŸ³ä¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
                    text = text.replace('â™ª', '').replace('â™«', '').replace('', '')
                    text = ''.join(char for i, char in enumerate(text) if char != text[i-1:i])
                    
                    # ç§»é™¤é‡å¤çš„å­—æ¯
                    import re
                    text = re.sub(r'([A-Za-z])\1{2,}', r'\1', text)
                    
                    # ä¿®æ­£å¸¸è§çš„é”™è¯¯è¯†åˆ«
                    corrections = {
                        'æ²¿åšæ˜¥': 'ä¸¥ä¼¯æ‘',
                        'åœ°çƒé¢åš': 'ä¸¥ä¼¯æ‘',
                        'å°é”…': 'å°å›½',
                        'å£°åŠ¨': 'ç”ŸåŠ¨',
                        'ä¸¾ç¤º': 'ä¸¾ä¸–',
                        'åˆ†é˜…': 'åˆ†å²¸'
                    }
                    for wrong, right in corrections.items():
                        text = text.replace(wrong, right)
                    
                    # è§„èŒƒåŒ–æ ‡ç‚¹ç¬¦å·
                    text = text.replace('ã€‚ã€‚', 'ã€‚').replace(',,', ',').replace(',.', 'ã€‚')
                    text = text.replace('?,', 'ã€‚').replace('? ', 'ã€‚').replace('.,', 'ã€‚')
                    
                    # ç¡®ä¿å¥å­ä»¥åˆé€‚çš„æ ‡ç‚¹ç»“æŸ
                    if text and not text[-1] in 'ã€‚ï¼ï¼Ÿ':
                        text += 'ã€‚'
                    
                    return text.strip()
                
                # è¿‡æ»¤å’Œæ¸…ç†æ–‡æœ¬æ®µè½
                filtered_segments = []
                prev_text = ""
                for segment in result["segments"]:
                    text = clean_text(segment["text"])
                    # å¦‚æœå½“å‰æ–‡æœ¬ä¸æ˜¯ä¸Šä¸€ä¸ªçš„é‡å¤ï¼Œä¸”é•¿åº¦åˆé€‚ï¼Œä¸”ä¸å…¨æ˜¯æ ‡ç‚¹ç¬¦å·
                    if (text != prev_text and 
                        len(text) > 1 and 
                        len(text) < 100 and  # é¿å…è¿‡é•¿çš„æ®µè½
                        any(c.isalnum() for c in text)):  # ç¡®ä¿åŒ…å«å®é™…æ–‡å­—
                        
                        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä¸å‰ä¸€ä¸ªæ®µè½åˆå¹¶
                        if (filtered_segments and 
                            filtered_segments[-1]["end"] + 0.3 >= segment["start"] and
                            len(filtered_segments[-1]["text"] + text) < 100):
                            # åˆå¹¶ç›¸é‚»çš„çŸ­å¥
                            filtered_segments[-1]["text"] += text
                            filtered_segments[-1]["end"] = segment["end"]
                        else:
                            filtered_segments.append({
                                "text": text,
                                "start": segment["start"],
                                "end": segment["end"]
                            })
                        prev_text = text
                
                # åˆå¹¶æœ€ç»ˆæ–‡æœ¬
                transcript = " ".join(segment["text"] for segment in filtered_segments)
                segments = filtered_segments
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if audio_path.exists():
                    audio_path.unlink()
                
                results = {
                    "transcript": transcript,
                    "segments": segments
                }
                
                # ä¿å­˜ç»“æœåˆ°ç¼“å­˜
                self._save_cache(cache_path, results)
                return results
            else:
                self.logger.error("Whisperæ¨¡å‹æœªæ­£ç¡®åŠ è½½")
                return {
                    "transcript": "",
                    "segments": []
                }
            
        except Exception as e:
            self.logger.error(f"éŸ³é¢‘åˆ†æå¤±è´¥: {str(e)}")
            return {
                "transcript": "",
                "segments": []
            }
    
    def _calculate_file_md5(self, file_path: str, chunk_size=8192) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å€¼"""
        md5 = hashlib.md5()
        with open(file_path, 'rb') as f:
            while chunk := f.read(chunk_size):
                md5.update(chunk)
        return md5.hexdigest()
            
    def _merge_timeline_results(self, visual_results: dict, audio_results: dict) -> list:
        """åˆå¹¶è§†è§‰å’ŒéŸ³é¢‘ç»“æœåˆ°æ—¶é—´çº¿æ ¼å¼"""
        # åˆ›å»ºæ—¶é—´çº¿å­—å…¸ï¼Œç”¨äºåˆå¹¶åŒä¸€æ—¶é—´ç‚¹çš„äº‹ä»¶
        timeline_dict = {}
        
        # å¤„ç†è§†è§‰æ£€æµ‹ç»“æœ
        for obj in visual_results.get("objects", []):
            time_str = f"{int(obj['time'] // 60):02d}:{int(obj['time'] % 60):02d}"
            if time_str not in timeline_dict:
                timeline_dict[time_str] = {"time": time_str, "visual": [], "ocr": [], "audiotext": ""}
            timeline_dict[time_str]["visual"].append(obj["class"])
            
        # å¤„ç†OCRç»“æœ
        for text in visual_results.get("texts", []):
            time_str = f"{int(text['time'] // 60):02d}:{int(text['time'] % 60):02d}"
            if time_str not in timeline_dict:
                timeline_dict[time_str] = {"time": time_str, "visual": [], "ocr": [], "audiotext": ""}
            timeline_dict[time_str]["ocr"].append(text["text"])
            
        # å¤„ç†éŸ³é¢‘ç»“æœ
        if audio_results and "segments" in audio_results:
            for segment in audio_results["segments"]:
                time_str = f"{int(segment['start'] // 60):02d}:{int(segment['start'] % 60):02d}"
                if time_str not in timeline_dict:
                    timeline_dict[time_str] = {"time": time_str, "visual": [], "ocr": [], "audiotext": ""}
                timeline_dict[time_str]["audiotext"] = segment["text"]
        
        # è½¬æ¢å­—å…¸ä¸ºåˆ—è¡¨å¹¶å»é‡
        timeline_list = []
        for time_point in sorted(timeline_dict.keys()):
            entry = timeline_dict[time_point]
            # å»é‡å¹¶è½¬æ¢åˆ—è¡¨ä¸ºå­—ç¬¦ä¸²
            entry["visual"] = ", ".join(sorted(set(entry["visual"]))) if entry["visual"] else ""
            entry["ocr"] = ", ".join(sorted(set(entry["ocr"]))) if entry["ocr"] else ""
            timeline_list.append(entry)
            
        return timeline_list

    def _generate_summary(self, visual_results: dict, audio_results: dict) -> str:
        """ä½¿ç”¨é€šä¹‰åƒé—®æˆ–OpenAI APIç”Ÿæˆå†…å®¹æ‘˜è¦"""
        try:
            if self.dashscope_api_key:
                return self._generate_tongyi_summary(visual_results, audio_results)
            elif self.openai_api_key:
                return self._generate_openai_summary(visual_results, audio_results)
            else:
                return self._generate_basic_summary(visual_results, audio_results)
                
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {str(e)}")
            return self._generate_basic_summary(visual_results, audio_results)

    def _generate_tongyi_summary(self, visual_results: dict, audio_results: dict) -> str:
        """ä½¿ç”¨é€šä¹‰åƒé—®APIç”Ÿæˆå†…å®¹æ‘˜è¦"""
        try:
            # ç”Ÿæˆè¯¦ç»†çš„æ—¶é—´çº¿æ•°æ®
            timeline_data = []
            
            # æ·»åŠ è§†è§‰äº‹ä»¶
            for obj in visual_results["objects"]:
                timeline_data.append({
                    "timestamp": obj["time"],
                    "type": "visual",
                    "content": f"æ£€æµ‹åˆ°{obj['class']}ï¼ˆç½®ä¿¡åº¦ï¼š{obj['confidence']:.2f}ï¼‰"
                })
            
            # æ·»åŠ éŸ³é¢‘äº‹ä»¶
            if audio_results and "segments" in audio_results:
                for segment in audio_results["segments"]:
                    timeline_data.append({
                        "timestamp": segment["start"],
                        "type": "audio",
                        "content": segment["text"]
                    })
            
            # æŒ‰æ—¶é—´æ’åº
            timeline_data.sort(key=lambda x: x["timestamp"])
            
            # ç”Ÿæˆæ—¶é—´çº¿æ–‡æœ¬
            timeline_text = []
            current_second = 0
            events_in_second = []
            
            for event in timeline_data:
                second = int(event["timestamp"])
                if second != current_second and events_in_second:
                    # åˆå¹¶åŒä¸€ç§’å†…çš„äº‹ä»¶
                    timeline_text.append(f"ç¬¬{current_second}ç§’ï¼š{'ï¼›'.join(events_in_second)}")
                    events_in_second = []
                current_second = second
                if event["type"] == "visual":
                    events_in_second.append(f"ç”»é¢ä¸­{event['content']}")
                else:
                    events_in_second.append(f"éŸ³é¢‘ï¼š{event['content']}")
            
            # æ·»åŠ æœ€åä¸€ç§’çš„äº‹ä»¶
            if events_in_second:
                timeline_text.append(f"ç¬¬{current_second}ç§’ï¼š{'ï¼›'.join(events_in_second)}")

            # å‡†å¤‡æç¤ºä¿¡æ¯
            prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹è§†é¢‘åˆ†ææ•°æ®ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„è§†é¢‘å†…å®¹æ—¶åºæè¿°æŠ¥å‘Šã€‚

è§†é¢‘åŸºæœ¬ä¿¡æ¯ï¼š
- è§†é¢‘æ€»æ—¶é•¿ï¼š{visual_results['metadata']['total_frames'] / visual_results['metadata']['fps']:.1f}ç§’
- åˆ†æå¸§æ•°ï¼š{visual_results['metadata']['total_frames']}å¸§
- æ£€æµ‹åˆ°çš„æ€»ç‰©ä½“æ•°ï¼š{len(visual_results['objects'])}ä¸ª
- ç‰©ä½“ç±»åˆ«æ•°ï¼š{len(set(obj['class'] for obj in visual_results['objects']))}ç§

è¯¦ç»†æ—¶é—´çº¿ï¼š
{chr(10).join(timeline_text[:50])}  # é™åˆ¶å‰50ä¸ªæ—¶é—´ç‚¹é¿å…è¶…å‡ºtokené™åˆ¶

è¯·ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„è§†é¢‘å†…å®¹åˆ†ææŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
1. æŒ‰ç…§æ—¶é—´é¡ºåºæè¿°è§†é¢‘ä¸­çš„ä¸»è¦åœºæ™¯å˜åŒ–å’Œé‡è¦äº‹ä»¶
2. é‡ç‚¹è¯´æ˜æ¯ä¸ªåœºæ™¯ä¸­å‡ºç°çš„ä¸»è¦äººç‰©å’Œç‰©ä½“ï¼Œä»¥åŠä»–ä»¬çš„äº’åŠ¨
3. ç»“åˆéŸ³é¢‘å†…å®¹ï¼Œè§£é‡Šè§†é¢‘ç”»é¢ä¸æ—ç™½/å¯¹è¯çš„å…³ç³»
4. æ€»ç»“è§†é¢‘çš„æ•´ä½“å™äº‹ç»“æ„å’Œä¸»è¦å†…å®¹ä¸»é¢˜

è¯·ç”¨æµç•…çš„è¯­è¨€æè¿°ï¼Œçªå‡ºæ—¶é—´é¡ºåºï¼Œè®©è¯»è€…èƒ½æ¸…æ™°ç†è§£è§†é¢‘çš„å†…å®¹å‘å±•è„‰ç»œã€‚"""

            # è°ƒç”¨é€šä¹‰åƒé—®API
            self.logger.info("æ­£åœ¨è°ƒç”¨é€šä¹‰åƒé—®APIç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ç”Ÿæˆè¯¦ç»†çš„è§†é¢‘å†…å®¹åˆ†ææŠ¥å‘Šã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]
            
            try:
                import requests.exceptions
                from urllib3.exceptions import ReadTimeoutError
                from socket import timeout
                
                # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º30ç§’
                response = dashscope.Generation.call(
                    model='qwen-max',
                    messages=messages,
                    result_format='message',
                    timeout=30,  # è®¾ç½®è¶…æ—¶æ—¶é—´
                    max_tokens=1500,  # é™åˆ¶ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦
                    temperature=0.7,  # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„åˆ›é€ æ€§
                    top_p=0.8  # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§
                )
                
                if response.status_code == HTTPStatus.OK:
                    return response.output.choices[0]['message']['content']
                else:
                    error_msg = f"é€šä¹‰åƒé—®APIè°ƒç”¨å¤±è´¥: {response.code} - {response.message}"
                    self.logger.error(error_msg)
                    return self._generate_basic_summary(visual_results, audio_results)
                    
            except (requests.exceptions.Timeout, ReadTimeoutError, timeout, KeyboardInterrupt) as e:
                self.logger.error(f"é€šä¹‰åƒé—®APIè°ƒç”¨è¶…æ—¶æˆ–è¢«ä¸­æ–­: {str(e)}")
                return self._generate_basic_summary(visual_results, audio_results)
                
            except Exception as e:
                self.logger.error(f"é€šä¹‰åƒé—®APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
                return self._generate_basic_summary(visual_results, audio_results)
                
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ‘˜è¦æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return self._generate_basic_summary(visual_results, audio_results)

    def _format_object_statistics(self, objects: list) -> str:
        """æ ¼å¼åŒ–ç‰©ä½“æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯"""
        object_counts = {}
        for obj in objects:
            key = obj["class"]
            if key not in object_counts:
                object_counts[key] = {
                    "count": 0,
                    "timestamps": []
                }
            object_counts[key]["count"] += 1
            object_counts[key]["timestamps"].append(obj["time"])
        
        stats = []
        for obj, data in sorted(object_counts.items(), key=lambda x: x[1]["count"], reverse=True):
            timestamps = sorted(data["timestamps"])
            first_appear = min(timestamps)
            last_appear = max(timestamps)
            stats.append(f"- {obj}: å‡ºç°{data['count']}æ¬¡ï¼Œé¦–æ¬¡å‡ºç°äº{first_appear:.1f}ç§’ï¼Œæœ€åå‡ºç°äº{last_appear:.1f}ç§’")
        
        return "\n".join(stats)

    def _format_timeline(self, visual_objects: list, audio_segments: list) -> str:
        """æ ¼å¼åŒ–æ—¶é—´çº¿
        
        Args:
            visual_objects (list): è§†è§‰å¯¹è±¡åˆ—è¡¨
            audio_segments (list): éŸ³é¢‘ç‰‡æ®µåˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–çš„æ—¶é—´çº¿
        """
        timeline = []
        
        # æ·»åŠ è§†è§‰æ£€æµ‹ç»“æœåˆ°æ—¶é—´çº¿
        for obj in visual_objects:
            timeline.append({
                "timestamp": obj["time"],
                "type": "visual",
                "content": f"æ£€æµ‹åˆ°{obj['class']}ï¼ˆç½®ä¿¡åº¦ï¼š{obj['confidence']:.2f}ï¼‰"
            })
            
        # æ·»åŠ æ–‡å­—è¯†åˆ«ç»“æœåˆ°æ—¶é—´çº¿
        for text in visual_objects.get("texts", []):
            timeline.append({
                "timestamp": text["time"],
                "type": "text",
                "content": f"è¯†åˆ«åˆ°æ–‡å­—ï¼š{text['text']}ï¼ˆç½®ä¿¡åº¦ï¼š{text['confidence']:.2f}ï¼‰"
            })
        
        # æ·»åŠ éŸ³é¢‘ç‰‡æ®µåˆ°æ—¶é—´çº¿
        for segment in audio_segments:
            timeline.append({
                "timestamp": segment["start"],
                "type": "audio",
                "content": segment["text"]
            })
            
        # æŒ‰æ—¶é—´æˆ³æ’åº
        timeline.sort(key=lambda x: x["timestamp"])
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted_timeline = []
        for event in timeline:
            timestamp = event["timestamp"]
            minutes = int(timestamp // 60)
            seconds = int(timestamp % 60)
            formatted_time = f"{minutes:02d}:{seconds:02d}"
            
            if event["type"] == "visual":
                prefix = "ğŸ‘"
            elif event["type"] == "text":
                prefix = "ğŸ“"
            else:
                prefix = "ğŸ”Š"
                
            formatted_timeline.append(f"{formatted_time} {prefix} {event['content']}")
            
        return "\n".join(formatted_timeline)

    def _generate_openai_summary(self, visual_results: dict, audio_results: dict) -> str:
        """ä½¿ç”¨OpenAI APIç”Ÿæˆå†…å®¹æ‘˜è¦"""
        try:
            detected_objects = set(obj["class"] for obj in visual_results["objects"])
            prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªç®€æ´çš„è§†é¢‘å†…å®¹æ‘˜è¦ï¼š

è§†è§‰å†…å®¹ï¼š
- æ£€æµ‹åˆ°çš„ç‰©ä½“ï¼š{', '.join(detected_objects)}
- åˆ†æçš„å¸§æ•°ï¼š{visual_results['metadata']['total_frames']}

éŸ³é¢‘å†…å®¹ï¼š
{audio_results['transcript']}

è¯·ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ‘˜è¦ï¼Œæè¿°è§†é¢‘çš„ä¸»è¦å†…å®¹å’Œé‡è¦äº‹ä»¶ã€‚"""

            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æåŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ]
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            self.logger.error(f"OpenAIæ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
            return self._generate_basic_summary(visual_results, audio_results)

    def _generate_basic_summary(self, visual_results: dict, audio_results: dict) -> str:
        """ç”ŸæˆåŸºæœ¬çš„å†…å®¹æ‘˜è¦"""
        try:
            if not visual_results or not isinstance(visual_results, dict):
                visual_results = {
                    "objects": [],
                    "texts": [],
                    "metadata": {
                        "total_frames": 0,
                        "fps": 0,
                        "width": 0,
                        "height": 0
                    }
                }
                
            if not audio_results or not isinstance(audio_results, dict):
                audio_results = {
                    "segments": []
                }
            
            # ç»Ÿè®¡ç‰©ä½“æ£€æµ‹ç»“æœ
            object_stats = self._format_object_statistics(visual_results.get("objects", []))
            
            # ç”Ÿæˆæ—¶é—´çº¿
            timeline = self._format_timeline(visual_results, audio_results.get("segments", []))
            
            # ç”Ÿæˆæ‘˜è¦æ–‡æœ¬
            summary = f"""è§†é¢‘åˆ†ææŠ¥å‘Šï¼š

åŸºæœ¬ä¿¡æ¯ï¼š
- è§†é¢‘æ—¶é•¿ï¼š{visual_results['metadata']['total_frames'] / visual_results['metadata']['fps']:.1f}ç§’
- è§†é¢‘åˆ†è¾¨ç‡ï¼š{visual_results['metadata']['width']}x{visual_results['metadata']['height']}
- æ£€æµ‹åˆ°çš„ç‰©ä½“æ•°ï¼š{len(visual_results.get('objects', []))}
- è¯†åˆ«åˆ°çš„æ–‡å­—æ•°ï¼š{len(visual_results.get('texts', []))}
- éŸ³é¢‘ç‰‡æ®µæ•°ï¼š{len(audio_results.get('segments', []))}

ç‰©ä½“æ£€æµ‹ç»Ÿè®¡ï¼š
{object_stats}

è¯¦ç»†æ—¶é—´çº¿ï¼š
{timeline}
"""
            return summary
            
        except Exception as e:
            self.logger.error(f"ç”ŸæˆåŸºæœ¬æ‘˜è¦æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return "æ— æ³•ç”Ÿæˆè§†é¢‘åˆ†ææ‘˜è¦ã€‚"

    def _time_to_seconds(self, time_str):
        """å°†æ—¶é—´å­—ç¬¦ä¸²ï¼ˆMM:SSï¼‰è½¬æ¢ä¸ºç§’æ•°"""
        try:
            minutes, seconds = map(int, time_str.split(':'))
            return minutes * 60 + seconds
        except Exception as e:
            self.logger.error(f"æ—¶é—´è½¬æ¢å¤±è´¥: {str(e)}")
            return 0 